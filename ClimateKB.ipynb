{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc57f0ed",
   "metadata": {},
   "source": [
    "## Generating a Climate Change Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e916eb",
   "metadata": {},
   "source": [
    "### 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80b586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Config as CON\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import penman\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "import amrlib\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#Declare some necessary global functions and objects\n",
    "stog = amrlib.load_stog_model()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "current_time_ms = lambda: int(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1b880",
   "metadata": {},
   "source": [
    "## 2. Read keyword frequency and identify top keywords (candidate entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd68cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I have already read the corpus, and computed frequency (in how many paper it appears) of each author-provided-keyword.\n",
    "'''\n",
    "keyword_filename = os.path.join(CON.OUTPUT_DATA_DIRECTORY,\"keyword_frequency.txt\")\n",
    "assert(os.path.exists(keyword_filename))\n",
    "\n",
    "with open(keyword_filename, encoding='utf-8') as f:\n",
    "    keyword_frequency = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880f4760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Top keywords are the ones that cover 80% of total keyword appearances.\n",
    "keyword_frequency[x] = how many times keyword x appeared in the corpus\n",
    "'''\n",
    "i = 0\n",
    "sum_frequency = 0\n",
    "freq_list = []\n",
    "\n",
    "for key in keyword_frequency.keys():\n",
    "    i +=1\n",
    "    f = keyword_frequency[key]\n",
    "    freq_list.append(f)\n",
    "    sum_frequency += f\n",
    "\n",
    "top_keywords = []\n",
    "cumulated_frequency = 0\n",
    "for key in keyword_frequency.keys():\n",
    "    f = keyword_frequency[key]\n",
    "    cumulated_frequency += f\n",
    "    top_keywords.append(key)\n",
    "    #Set it to 0.6 (60%) for a quick run\n",
    "    if cumulated_frequency>(0.8*sum_frequency):\n",
    "        break\n",
    "\n",
    "'''\n",
    "Try to discard repeated appearance of the same keyword in a different form\n",
    "'''\n",
    "top_keywords = list(set([wordnet_lemmatizer.lemmatize(k.lower()) for k in top_keywords]))\n",
    "\n",
    "plt.plot(np.arange(0,len(top_keywords)),freq_list[0:len(top_keywords)]); #Show frequency distribution of top keywords\n",
    "plt.xlabel('n (n-th frequent keyword)')\n",
    "plt.ylabel('Keyword frequency')\n",
    "\n",
    "#Write top keywords to a file\n",
    "with open('top_keywords.txt', 'w') as f:\n",
    "    for item in top_keywords:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde3f882",
   "metadata": {},
   "source": [
    "## 3. Train a syntactic parser with ability of chunking phrases\n",
    "\n",
    "Also define some helper functions to read the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b21c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  90.0%%\n",
      "    Precision:     82.1%%\n",
      "    Recall:        86.3%%\n",
      "    F-Measure:     84.1%%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train a phrase-chunking mode based on CONLL-2000 corpus\n",
    "'''\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.chunk.util import tree2conlltags,conlltags2tree\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "data= conll2000.chunked_sents()\n",
    "train_data=data[:10900]\n",
    "test_data=data[10900:]\n",
    "\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "#Define the chunker class\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):\n",
    "        train_sent_tags=conll_tag_chunks(train_sentences)\n",
    "        self.chunk_tagger=combined_tagger(train_sent_tags,tagger_classes)\n",
    "        \n",
    "    def parse(self,tagged_sentence):\n",
    "        if not tagged_sentence:\n",
    "            return None\n",
    "        pos_tags=[tag for word, tag in tagged_sentence]\n",
    "        chunk_pos_tags=self.chunk_tagger.tag(pos_tags)\n",
    "        chunk_tags=[chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]\n",
    "        wpc_tags=[(word,pos_tag,chunk_tag) for ((word,pos_tag),chunk_tag) in zip(tagged_sentence,chunk_tags)]\n",
    "        return conlltags2tree(wpc_tags)\n",
    "\n",
    "#train chunker model\n",
    "ntc=NGramTagChunker(train_data)\n",
    "#evaluate chunker model performance\n",
    "print(ntc.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5103db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(t):\n",
    "    '''\n",
    "    Given a parse tree t, convert it to a string for visualization\n",
    "    '''\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        return t[0]\n",
    "    else:\n",
    "        txt = \"\"\n",
    "        for child in t:\n",
    "            txt = txt + print_tree(child).strip() + \" \"\n",
    "        return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8eca00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(t):\n",
    "    '''\n",
    "    Input:\n",
    "        A syntactic parse tree, t\n",
    "    Returns:\n",
    "        List of noun phrases (chunked) -- are possible entities\n",
    "        List of verb phrases (chunked) -- are possible relations\n",
    "    '''\n",
    "    noun_phrases = []\n",
    "    verb_phrases = []\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        return ([],[])\n",
    "        \n",
    "    else:\n",
    "        if(t.label()=='NP'):\n",
    "            noun_phrases.append(print_tree(t))\n",
    "            \n",
    "        if(t.label()=='VP'):\n",
    "            verb_phrases.append(print_tree(t))\n",
    "        \n",
    "        for child in t:\n",
    "            np, vp = traverse(child)\n",
    "            noun_phrases.extend(np)\n",
    "            verb_phrases.extend(vp)\n",
    "            \n",
    "    return noun_phrases, verb_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53a0b5",
   "metadata": {},
   "source": [
    "## 4. Helper functions to modify the triplets found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be445500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(word, from_pos=\"v\", to_pos=\"n\"):    \n",
    "    \"\"\" \n",
    "        Transform words given from/to POS tags.\n",
    "        The assumption is, in a (subject, object, verb) triplet, subject and object should be noun-phrases\n",
    "    \"\"\"\n",
    "    \n",
    "    WN_NOUN = 'n'\n",
    "    WN_VERB = 'v'\n",
    "    WN_ADJECTIVE = 'a'\n",
    "    WN_ADJECTIVE_SATELLITE = 's'\n",
    "    WN_ADVERB = 'r'\n",
    "\n",
    "    # If the word is already in desired form, no need to modify\n",
    "    synsets = wn.synsets(word, pos=to_pos)\n",
    "    if synsets:\n",
    "        return word\n",
    "    \n",
    "    synsets = wn.synsets(word, pos=from_pos)\n",
    "\n",
    "    # Word not found, perhaps the word is not a verb, and maybe already a noun\n",
    "    if not synsets:\n",
    "        return word\n",
    "\n",
    "    # Get all lemmas of the word (consider 'a'and 's' equivalent)\n",
    "    lemmas = []\n",
    "    for s in synsets:\n",
    "        for l in s.lemmas():\n",
    "            if s.name().split('.')[1] == from_pos or from_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and s.name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "                lemmas += [l]\n",
    "\n",
    "    # Get related forms\n",
    "    derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
    "\n",
    "    # filter only the desired pos (consider 'a' and 's' equivalent)\n",
    "    related_noun_lemmas = []\n",
    "\n",
    "    for drf in derivationally_related_forms:\n",
    "        for l in drf[1]:\n",
    "            if l.synset().name().split('.')[1] == to_pos or to_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and l.synset().name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "                related_noun_lemmas += [l]\n",
    "\n",
    "    # Extract the words from the lemmas\n",
    "    words = [l.name() for l in related_noun_lemmas]\n",
    "    len_words = len(words)\n",
    "\n",
    "    # Build the result in the form of a list containing tuples (word, probability)\n",
    "    result = [(w, float(words.count(w)) / len_words) for w in set(words)]\n",
    "    result.sort(key=lambda w:-w[1])\n",
    "    \n",
    "    #Get the noun that is closer to the word syntactically (edit distance) and matches symantically\n",
    "    d = 10000\n",
    "    i = 0\n",
    "    res = \"\"\n",
    "    for entry in result:\n",
    "        '''\n",
    "        when converting from verb to noun, insertion is natural\n",
    "        entry[1] is the probability/semantic similarity, so (1-entry[1]) can be a distance\n",
    "        '''\n",
    "        edit_distance = (1-entry[1])*Levenshtein.distance(word,entry[0],weights=(0.5,1,1))\n",
    "        if d>edit_distance:\n",
    "            d = edit_distance\n",
    "            res = entry[0]\n",
    "        if i==3:\n",
    "            break\n",
    "\n",
    "    # return all the possibilities sorted by probability\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f573e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification\n",
      "attestation\n",
      "modification\n",
      "die\n",
      "deforestation\n",
      "maintainer\n",
      "water\n"
     ]
    }
   ],
   "source": [
    "#unit test\n",
    "print(convert(\"verify\"))\n",
    "print(convert(\"attestation\"))\n",
    "print(convert(\"modify\"))\n",
    "print(convert(\"die\"))\n",
    "print(convert(\"deforest\"))\n",
    "print(convert(\"maintain\"))\n",
    "print(convert(\"water\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af5f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_tuple(sub,obj,rel,nps):\n",
    "    '''\n",
    "    Final level modification to a (subject, object, relation) tuple\n",
    "    Inputs:\n",
    "        (sub,obj,rel): triplet obtained from semantic parser\n",
    "        nps: noun phrases obtained from a syntactic parse\n",
    "        \n",
    "    Outputs:\n",
    "        (sub,obj,rel): remove AMR specific codecs and convert subject and object to its noun form (whenever possible)\n",
    "    '''\n",
    "    \n",
    "    #discard AMR specific tags (cause-01 -> cause)\n",
    "    if \"-0\" in sub:\n",
    "        sub = sub[0:sub.find(\"-0\")]\n",
    "\n",
    "    if \"-0\" in obj:\n",
    "        obj = obj[0:obj.find(\"-0\")]\n",
    "\n",
    "    if \"-0\" in rel:\n",
    "        rel = rel[0:rel.find(\"-0\")]\n",
    "        \n",
    "\n",
    "    sub_nps = []\n",
    "    obj_nps = []\n",
    "    for np in nps:\n",
    "        #If subject or object matches the postfix of any of the noun phrases, that noun phrase becomes the new subject/object\n",
    "        if wordnet_lemmatizer.lemmatize(np.split(\" \")[-1].lower()) == wordnet_lemmatizer.lemmatize(sub.lower()):\n",
    "            sub_nps.append(np)\n",
    "\n",
    "        if wordnet_lemmatizer.lemmatize(np.split(\" \")[-1].lower()) == wordnet_lemmatizer.lemmatize(obj.lower()):\n",
    "            obj_nps.append(np)\n",
    "\n",
    "    '''\n",
    "    Subject and object should be nouns\n",
    "    '''\n",
    "    sub = convert(sub)\n",
    "    obj = convert(obj)\n",
    "    \n",
    "    if len(sub_nps)==1:\n",
    "        sub = sub_nps[0]\n",
    "\n",
    "    if len(obj_nps)==1:\n",
    "        obj = obj_nps[0]\n",
    "\n",
    "        \n",
    "    return (sub,obj,rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b926fb",
   "metadata": {},
   "source": [
    "## 5. Process an abstract and generate triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2231b91f",
   "metadata": {},
   "source": [
    "### 5.1 Helper functions for processing sentences with AMR Semantic Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f0a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_entry(D, entities, src):\n",
    "    '''\n",
    "    Inputs:\n",
    "        D: Semantic parse tree in a json (dictionary format)\n",
    "        entities: variable-lexicon mapping for the parse tree entries\n",
    "        src: a variable in the parse tree\n",
    "        \n",
    "    Output:\n",
    "        string -- the appropriate lexicon for the \"src\" variable\n",
    "    '''\n",
    "    entity_base_str = entities[src]\n",
    "    if \"-0\" in entity_base_str:\n",
    "        entity_base_str = entity_base_str[0:entity_base_str.find(\"-0\")]\n",
    "        \n",
    "    entity_str = \"\"\n",
    "    \n",
    "    if src in D.keys():\n",
    "        #Process conjunction (and/or)\n",
    "        if ((entity_base_str==\"and\") or (entity_base_str==\"or\")):\n",
    "            options = [op for op in D[src].keys() if \":op\" in op]\n",
    "            \n",
    "            if len(options)!=0:\n",
    "                entity_str = show_entry(D,entities,D[src][options[0]])\n",
    "                for op in options[1:]:\n",
    "                    entity_str+=\" \"+entity_base_str+\" \"\n",
    "                    entity_str+=convert(show_entry(D,entities,D[src][op]))\n",
    "                    \n",
    "        elif (entity_base_str==\"name\"):\n",
    "            options = [op for op in D[src].keys() if \":op\" in op]\n",
    "            \n",
    "            if len(options)!=0:\n",
    "                entity_str = show_entry(D,entities,D[src][options[0]])\n",
    "                for op in options[1:]:\n",
    "                    entity_str+=\" \"\n",
    "                    entity_str+=show_entry(D,entities,D[src][op])    \n",
    "                \n",
    "            \n",
    "        #Process :mod relation\n",
    "        elif \":mod\" in D[src].keys():  \n",
    "            entity_str = convert(entity_base_str)\n",
    "            if D[src][\":mod\"] in D.keys() and \":mod\" in D[D[src][\":mod\"]].keys():\n",
    "                entity_str =entities[D[D[src][\":mod\"]][\":mod\"]]+\" \"+entity_str\n",
    "            else:\n",
    "                entity_str = entities[D[src][\":mod\"]]+\" \"+entity_str\n",
    "    \n",
    "        else:\n",
    "            entity_str = convert(entity_base_str)\n",
    "    else:\n",
    "        entity_str = convert(entity_base_str)\n",
    "    #entity_str += entities[src]\n",
    "    #print(entity_base_str, entity_str)\n",
    "    #print(entity_base_str)\n",
    "    return entity_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c3ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amr_entailment(g, e1, e2):\n",
    "    '''\n",
    "            *****\n",
    "            Note: Ended up not using it. \n",
    "            So this is not fine-tuned, and may not be consistent with the rest of the code.\n",
    "            AMR parser does not do a great job on complex, academic text to check whether it entains a triplet.\n",
    "            *****\n",
    "    \n",
    "    inputs:\n",
    "        - g: AMR graph\n",
    "        - e1: entity 1\n",
    "        - e2: entity 2\n",
    "        \n",
    "    outputs:\n",
    "        - enails: Check whether the AMR graph has a node with e1 and e2 as arguments.\n",
    "        - triplets: (subject, object, relation); The node instance (relation) that has e1 and e2 as arguments\n",
    "    '''\n",
    "    triplets = []\n",
    "    \n",
    "    entities = {}\n",
    "    gg = penman.decode(g)\n",
    "    rels = []\n",
    "    \n",
    "    #Map the variables to their corresponding lexicons\n",
    "    for (src, role, target) in gg.instances():\n",
    "        entities[src] = target\n",
    "    \n",
    "    #Find a node that relates to the input subject (e1)\n",
    "    for (src, role, target) in gg.edges():\n",
    "        t = entities[target]\n",
    "        #AMR adds additional numbers to represent a lexicon uniquely (e.g., burn -> burn-01). \n",
    "        #Remove this to get to the root form\n",
    "        if \"-0\" in t:\n",
    "            t = t[0:t.find(\"-0\")]\n",
    "            \n",
    "        t = wordnet_lemmatizer.lemmatize(t.lower())\n",
    "        \n",
    "        #e1 is an argument of the relational verb (src)\n",
    "        if t==e1 and role in [\":ARG0\",\":ARG1\"]:\n",
    "            rels.append((src,role))\n",
    "    \n",
    "    #No candidate relational verb found\n",
    "    if len(rels)==0:\n",
    "        return (False, triplets)\n",
    "\n",
    "    #For each candidate relational verb, verify that e2 is also its argument\n",
    "    for (rel_src,rel_role) in rels:\n",
    "        for (src, role, target) in gg.edges():\n",
    "\n",
    "            #if e1 is arg1, then e2 should be arg2, and likewise if e1 is arg2\n",
    "            des_role = \":ARG0\"\n",
    "            if rel_role==\":ARG0\":\n",
    "                des_role = \":ARG1\"\n",
    "\n",
    "            #reduce target lexicon to its root form\n",
    "            t = entities[target]\n",
    "            if \"-0\" in t:\n",
    "                t = t[0:t.find(\"-0\")]\n",
    "                \n",
    "            t = wordnet_lemmatizer.lemmatize(t.lower())\n",
    "\n",
    "            #verify that e2 is another argument for the relational verb\n",
    "            if (src==rel_src) and (role==des_role) and (t==e2):\n",
    "                src_str = entities[src]\n",
    "                if \"-0\" in src_str:\n",
    "                    src_str = src_str[0:src_str.find(\"-0\")]\n",
    "\n",
    "                #and is typically not a relation, rather shows that two arguments are probably related\n",
    "                if src_str==\"and\":\n",
    "                    src_str = \"are related\"\n",
    "\n",
    "                if des_role==\":ARG0\":\n",
    "                    triplets.append((e2, e1, src_str))\n",
    "                else:\n",
    "                    triplets.append((e1, e2, src_str))\n",
    "    \n",
    "    #If no triplet found, AMR fails to find any relation between e1 and e2\n",
    "    if len(triplets)==0:\n",
    "        return (False, triplets)\n",
    "    \n",
    "    #Otherwise return the triplets\n",
    "    return (True, triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4129028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amr_triplets(g, nps):\n",
    "    '''\n",
    "    inputs:\n",
    "        - g: AMR graph\n",
    "        - nps: a list of noun phrases (extracted using a syntactic parser)\n",
    "    outputs:\n",
    "        - triplets (subject, object, relation)\n",
    "    '''\n",
    "    \n",
    "    triplets = []\n",
    "    entities = {}\n",
    "    gg = penman.decode(g)\n",
    "    \n",
    "    #Map the variables to their corresponding lexicons\n",
    "    for (src, role, target) in gg.instances():\n",
    "        entities[src] = target\n",
    "        \n",
    "    D = {}\n",
    "    #Find a node that relates to the input subject (e1)\n",
    "    for (src, role, target) in gg.edges():\n",
    "        if src in D.keys():\n",
    "            D[src][role] = target\n",
    "        else:\n",
    "            D[src] = {}\n",
    "            D[src][role] = target\n",
    "    \n",
    "    #print(g)\n",
    "    #print(entities)\n",
    "    #print(D)\n",
    "    \n",
    "           \n",
    "    for src in D.keys():\n",
    "        \n",
    "        '''\n",
    "        :name\n",
    "        process specification\n",
    "        '''\n",
    "        src_alias = None\n",
    "        if \":name\" in D[src].keys():\n",
    "            src_alias = D[src][\":name\"]\n",
    "        '''\n",
    "        Process ARG semantic type\n",
    "        ''' \n",
    "        if \":ARG0\" in D[src].keys() and \":ARG1\" in D[src].keys():\n",
    "            \n",
    "            sub = show_entry(D, entities, D[src][\":ARG0\"])\n",
    "            \n",
    "            obj = show_entry(D, entities, D[src][\":ARG1\"])\n",
    "            if src_alias is None:\n",
    "                rel = entities[src]\n",
    "            else:\n",
    "                rel = entities[src_alias]\n",
    "                        \n",
    "            triplets.append(fix_tuple(sub,obj,rel,nps))\n",
    "            \n",
    "        #elif \":ARG1\" in D[src].keys() and \":ARG2\" in D[src].keys():\n",
    "            \n",
    "            #sub = show_entry(D, entities, D[src][\":ARG1\"])\n",
    "            #obj = show_entry(D, entities, D[src][\":ARG2\"])\n",
    "            #rel = show_entry(D, entities, src)\n",
    "                        \n",
    "            #triplets.append(fix_tuple(sub,obj,rel,nps))\n",
    "        \n",
    "        '''\n",
    "        :ARG1-of / cause-01\n",
    "        penman handles \"/d :ARG1-of /c cause-01\" as \":ARG0 :ARG1\" pattern\n",
    "        '''\n",
    "        #if \":ARG1-of\" in D[src].keys() and entities[D[src][\":ARG1-of\"]]==\"cause-01\":\n",
    "        #    print((\"*\"*5)+\"CAUSE role\"+(\"*\"*5))\n",
    "        #    print(entities)\n",
    "        #    print(D[src])\n",
    "        \n",
    "        '''\n",
    "        :consist-of\n",
    "        panman reverses any -of role in AMR. \"A :consist-of B\" becomes \"B :consist A\"\n",
    "        '''\n",
    "        if \":consist\" in D[src].keys():\n",
    "            \n",
    "            sub = show_entry(D, entities, D[src][\":consist\"])\n",
    "            \n",
    "            if src_alias is None:\n",
    "                obj = show_entry(D, entities, src)\n",
    "            else:\n",
    "                obj = show_entry(D, entities, src_alias)\n",
    "            rel = \"contains\"\n",
    "            \n",
    "            triplets.append(fix_tuple(sub,obj,rel,nps))\n",
    "            \n",
    "        \n",
    "                \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b9910",
   "metadata": {},
   "source": [
    "### 5.2 Use AMR to generate triplets from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbf41e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Human/JJ activities/NNS)\n",
      "  (VP are/VBP increasing/VBG)\n",
      "  (PP in/IN)\n",
      "  (NP an/DT uncontrollable/JJ manner./NN))\n",
      "(S\n",
      "  (NP These/DT activities/NNS)\n",
      "  (VP are/VBP causing/VBG)\n",
      "  (NP climate/NN change./NN))\n",
      "(S\n",
      "  (NP The/DT old/JJ man/NN)\n",
      "  (VP died/VBD)\n",
      "  (PP of/IN)\n",
      "  (NP lung/NN cancer./NN))\n",
      "(S\n",
      "  (NP I/PRP)\n",
      "  (VP saw/VBD)\n",
      "  (NP a/DT cloud/NN)\n",
      "  (PP of/IN)\n",
      "  (NP dust./NN))\n",
      "(S\n",
      "  (NP A/DT team/NN)\n",
      "  (PP of/IN)\n",
      "  (NP researchers/NNS)\n",
      "  (VP made/VBD)\n",
      "  (NP a/DT discovery/NN)\n",
      "  (PP about/IN)\n",
      "  (NP a/DT ring/NN)\n",
      "  (PP of/IN)\n",
      "  (NP gold./NN))\n",
      "(S\n",
      "  (NP Deforestation/NN)\n",
      "  and/CC\n",
      "  (NP flood/NN)\n",
      "  (VP are/VBP)\n",
      "  (NP bad/JJ)\n",
      "  (PP for/IN)\n",
      "  (NP nature./NN))\n",
      "(S\n",
      "  (NP Steven/NNP Spielberg/NNP)\n",
      "  (VP lives/VBZ)\n",
      "  (PP in/IN)\n",
      "  (NP Los/NNP Angeles./NNP))\n",
      "(S (NP The/DT comment/NN) (VP is/VBZ) (NP inappropriate./JJ))\n",
      "Abstract: 1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Remove short keywords from the top keyword list\n",
    "'''\n",
    "temp = []\n",
    "for x in top_keywords:\n",
    "    #Keywords with a single or couple letters are almost meaningless\n",
    "    if len(x)>=3:\n",
    "        temp.append(x)\n",
    "top_keywords.clear()\n",
    "top_keywords = temp\n",
    "top_keywords_set = set(top_keywords)\n",
    "\n",
    "abstract_filename = os.path.join(CON.OUTPUT_DATA_DIRECTORY,\"example_abstract.json\")\n",
    "#abstract_filename = os.path.join(CON.OUTPUT_DATA_DIRECTORY,\"all_abstracts_with_keywords.json\")\n",
    "assert(os.path.exists(abstract_filename))\n",
    "\n",
    "with open(abstract_filename, encoding='utf-8') as f:\n",
    "    all_abstracts = json.loads(f.read())\n",
    "\n",
    "stop_after = 300000\n",
    "start_from = 0\n",
    "i = start_from\n",
    "graph = None\n",
    "all_triplets = []\n",
    "\n",
    "for abstract_with_keywords in all_abstracts[i:]:\n",
    "    i +=1\n",
    "    if (i%500 == 0):\n",
    "        '''\n",
    "        Write output triplets in a CSV file\n",
    "        '''\n",
    "        keys = all_triplets[0].keys()\n",
    "\n",
    "        a_file = open(\"kb_triplets_raw_\"+str(start_from)+\"_\"+str(i-1)+\".csv\", \"w\")\n",
    "        dict_writer = csv.DictWriter(a_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(all_triplets)\n",
    "        a_file.close()\n",
    "    \n",
    "    try:\n",
    "        keywords = abstract_with_keywords[\"keywords\"]\n",
    "        abstract = abstract_with_keywords[\"abstract\"]\n",
    "\n",
    "        #if the paper-keywords contains some top keywords, parse it with a semantic parser.\n",
    "        #else skip\n",
    "        if len(set(keywords).intersection(top_keywords_set))==0:\n",
    "            continue\n",
    "\n",
    "        if abstract == \"\":\n",
    "            continue\n",
    "\n",
    "\n",
    "        sentences = sent_tokenize(abstract)\n",
    "        #graphs contain AMR parse for each sentences of the abstract\n",
    "        graphs = None\n",
    "\n",
    "        j = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            #Extract parts of speech of words and chunk them into phrases\n",
    "            nltk_pos_tagged=nltk.pos_tag(sentence.split())\n",
    "            chunk_tree=ntc.parse(nltk_pos_tagged)\n",
    "            #print(chunk_tree)\n",
    "            #Extract noun and verb phrases from the sentence\n",
    "            nps, vps = traverse(chunk_tree)\n",
    "            nps = list(set(nps))\n",
    "\n",
    "            '''\n",
    "            Run AMR parser on all the sentences simultaneously\n",
    "            '''\n",
    "            if graphs is None:\n",
    "                graphs = stog.parse_sents(sent_tokenize(abstract))\n",
    "\n",
    "            graph = graphs[j]\n",
    "            #print(graph)\n",
    "            triplets = amr_triplets(graph, nps)\n",
    "\n",
    "            for item in triplets:\n",
    "                (sub, obj, rel) = item\n",
    "                #print(\"<\"+sub+\">, <\"+obj+\">, <\"+rel+\">\")\n",
    "                tr = {}\n",
    "                tr[\"subject\"] = sub\n",
    "                tr[\"object\"] = obj\n",
    "                tr[\"relation\"] = rel\n",
    "                all_triplets.append(tr)\n",
    "\n",
    "            j +=1\n",
    "\n",
    "\n",
    "    except:\n",
    "        print(\"An Exception!\")\n",
    "        \n",
    "    print(\"Abstract: \"+str(i))\n",
    "    \n",
    "    \n",
    "    if i==stop_after:\n",
    "        break\n",
    "\n",
    "#Save upon completion\n",
    "keys = all_triplets[0].keys()\n",
    "a_file = open(\"kb_triplets_raw_all.csv\", \"w\")\n",
    "dict_writer = csv.DictWriter(a_file, keys)\n",
    "dict_writer.writeheader()\n",
    "dict_writer.writerows(all_triplets)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08c4b2",
   "metadata": {},
   "source": [
    "## Major Limitations\n",
    "\n",
    "1. Cannot handle negative polarity (the penman parser I used ignores the polarity edges in the original AMR graph)\n",
    "2. After collecting all the triplets, we can analyze them and scrap the triplets that appears less frequently (more likely to be noise)\n",
    "3. AMR is not so good with the complex sentences found in academic papers. Simplifying the sentences before using AMR might improve the quality of the triplets.\n",
    "4. Summarizing a sub-parse-tree of AMR back to a sentence is challenging, and if done properly, it could make the knowledge graph way more useful. For example, rather than saying \"online resources may help improvement\", it might be more useful to say \"online resources may help improvement of teaching evolutionary theory\". In AMR parser, it is encoded as: help is the root-verb, :ARG0 being the subtree containing \"resources :mod online\", :ARG1 being the subtree containing the semantic aspects of \"improve the teaching of evolutionary theory in European secondary schools\", improvement being the root lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13538a1b",
   "metadata": {},
   "source": [
    "Note: from tuple, we can use GPT to generate simple sentences. For example, (\"air pollution\", \"cancer\", \"cause\") can be used to generate \"Air pollution may cause cancer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e81b7b",
   "metadata": {},
   "source": [
    "Additional Resource:\n",
    "\n",
    "AMR parser relevant code:\n",
    "https://github.com/snowblink14/smatch/blob/master/amr.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f9492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
